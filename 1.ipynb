{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66997,
     "status": "ok",
     "timestamp": 1543753477854,
     "user": {
      "displayName": "Heeyeon Kwon",
      "photoUrl": "",
      "userId": "16681195675344528845"
     },
     "user_tz": 480
    },
    "id": "GWcQZB32ezLu",
    "outputId": "97ccf74a-ad5c-4668-9fa2-fa90f99c1a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/hkwon214/.local/lib/python2.7/site-packages (0.2.0.post3)\n",
      "Requirement already satisfied: numpy in /home/hkwon214/.local/lib/python2.7/site-packages (from torch) (1.15.4)\n",
      "Requirement already satisfied: pyyaml in /home/hkwon214/.local/lib/python2.7/site-packages (from torch) (3.13)\n",
      "Requirement already satisfied: torchvision in /home/hkwon214/.local/lib/python2.7/site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/hkwon214/.local/lib/python2.7/site-packages (from torchvision) (5.3.0)\n",
      "Requirement already satisfied: torch in /home/hkwon214/.local/lib/python2.7/site-packages (from torchvision) (0.2.0.post3)\n",
      "Requirement already satisfied: six in /usr/lib/python2.7/dist-packages (from torchvision) (1.10.0)\n",
      "Requirement already satisfied: numpy in /home/hkwon214/.local/lib/python2.7/site-packages (from torchvision) (1.15.4)\n",
      "Requirement already satisfied: pyyaml in /home/hkwon214/.local/lib/python2.7/site-packages (from torch->torchvision) (3.13)\n",
      "Requirement already satisfied: tqdm in /home/hkwon214/.local/lib/python2.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: Pillow==5.3.0 in /home/hkwon214/.local/lib/python2.7/site-packages (5.3.0)\n",
      "Collecting PIL\n",
      "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
      "Requirement already satisfied: image in /home/hkwon214/.local/lib/python2.7/site-packages (1.5.27)\n",
      "Requirement already satisfied: django in /home/hkwon214/.local/lib/python2.7/site-packages (from image) (1.11.16)\n",
      "Requirement already satisfied: pillow in /home/hkwon214/.local/lib/python2.7/site-packages (from image) (5.3.0)\n",
      "Requirement already satisfied: pytz in /home/hkwon214/.local/lib/python2.7/site-packages (from django->image) (2018.7)\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Install pytorch and tqdm (if necessary)\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "\n",
    "!pip install Pillow==5.3.0\n",
    "!pip install PIL\n",
    "!pip install image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee9W_AEye9Ay"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named google.colab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ac8a6847b109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount your google drive as the data drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This will require google authorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named google.colab"
     ]
    }
   ],
   "source": [
    "# Mount your google drive as the data drive\n",
    "# This will require google authorization\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0exUPVzZ0OT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_iCTRY3aKSD"
   },
   "outputs": [],
   "source": [
    "!cat /etc/*-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNFsTMhTfF28"
   },
   "outputs": [],
   "source": [
    "# Handle imports\n",
    "\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import torch.utils.data as TUdata\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() \n",
    "                           #                          else torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSU2FCwyfOMT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def default_loader(path):\n",
    "\treturn Image.open(path).convert('RGB')\n",
    "\n",
    "class FOOD101(TUdata.Dataset):\n",
    "    def __init__(self, root, list_IDs, labels, transform=None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data_dir = os.path.join(root,'food101/images/')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label ##example drive/My Drive/cs482/DL-project/food101/apple_pie/xxxxx.jpg\n",
    "        img = default_loader(self.data_dir + ID[0] + '/' + ID[1] + '.jpg')\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(img)\n",
    "        y = self.labels[ID[0]]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOz6wBIY8tCE"
   },
   "source": [
    "TESTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDp6iDEa8zvL"
   },
   "outputs": [],
   "source": [
    "def partition_data_and_index_labels(data_dir, dataset, classes_txt, train_txt, test_txt):\n",
    "    labels_file = os.path.join(data_dir,dataset,classes_txt)\n",
    "    labels = {}\n",
    "    '''labels -> {apple_pie : 0, fish_and_chips : ...}'''\n",
    "    with open(labels_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            line = line\n",
    "            labels[line] = idx\n",
    "    \n",
    "    train_file = os.path.join(data_dir,dataset,train_txt)\n",
    "    test_file = os.path.join(data_dir,dataset,test_txt)\n",
    "    partition = defaultdict(list)\n",
    "    \n",
    "    '''partition -> {'train : [[apple_pie, xxxxx.jpg], [apple_pie, xxyyyy.jpg], \n",
    "    [fish_and_chips, aaaabb.jpg]], test: [[apple_pie, abcdabcd.jpg]]}}'''\n",
    "    \n",
    "    with open(train_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            ID = line.split('/')\n",
    "            partition['train'].append([ID[0], ID[1]])\n",
    "        \n",
    "\n",
    "    with open(test_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            ID = line.split('/')\n",
    "            partition['test'].append([ID[0], ID[1]])\n",
    "            \n",
    "    return partition, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rA_y2v_08vCI"
   },
   "outputs": [],
   "source": [
    "# Generators\n",
    "def prepare_dataset(args):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "    \n",
    "    \n",
    "    train_dataset = FOOD101(args.data_dir, args.partition['train'], args.labels, transform=transforms.Compose([transforms.RandomResizedCrop(args.resolution),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                               ]))\n",
    "\n",
    "\n",
    "    train_loader = TUdata.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "    test_dataset = FOOD101(args.data_dir, args.partition['test'], args.labels, transform=transforms.Compose([transforms.RandomResizedCrop(args.resolution),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                               ]))\n",
    "\n",
    "    test_loader = TUdata.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    \n",
    "    def time_stamp(fname, fmt='%m-%d-%H-%M_{fname}'):\n",
    "        return datetime.datetime.now().strftime(fmt).format(fname=fname)\n",
    "        \n",
    "    training_run_name = time_stamp(args.dataset + '_' + args.name)\n",
    "    training_run_dir = os.path.join(args.data_dir, 'run_dir', training_run_name)\n",
    "    \n",
    "    if not os.path.exists(training_run_dir):\n",
    "        os.makedirs(training_run_dir)\n",
    "    \n",
    "    return train_loader, test_loader, train_dataset, test_dataset, training_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2D5b456XHfR3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkKaDdvBt90u"
   },
   "outputs": [],
   "source": [
    "def cm_analysis(y_true, y_pred, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    labels=map(str, range(9 + 1))\n",
    "    labels_file = os.path.join(args.data_dir,args.dataset,'meta/C10.txt')\n",
    "    ymap = {}\n",
    "    '''labels -> {apple_pie : 0, fish_and_chips : 4}'''\n",
    "    with open(labels_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            line = line\n",
    "            ymap[idx] = line\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[np.int(yi)] for yi in y_pred]\n",
    "        y_true = [ymap[np.int(yi)] for yi in y_true]\n",
    "        labels = [ymap[np.int(yi)] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
    "    plt.show\n",
    "    #plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kySN9U_Fijpr"
   },
   "outputs": [],
   "source": [
    "# The Args object will contain all of our parameters\n",
    "# If you want to run with different arguments, create another Args object\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self, name='food101', batch_size=64, test_batch_size=1000,\n",
    "            epochs=50, lr=0.0001, optimizer='adam', momentum=0.5,\n",
    "            seed=1, log_interval=100, dataset='food101', n_categories=101,\n",
    "            data_dir='', model='ResNet101',\n",
    "            cuda=True):\n",
    "        self.name = name # name for this training run. Don't use spaces.\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size # Input batch size for testing\n",
    "        self.epochs = epochs # Number of epochs to train\n",
    "        self.lr = lr # Learning rate\n",
    "        self.optimizer = optimizer # sgd/p1sgd/adam/rms_prop\n",
    "        self.momentum = momentum # SGD Momentum\n",
    "        self.seed = seed # Random seed\n",
    "        self.log_interval = log_interval # Batches to wait before logging\n",
    "                                     # detailed status. 0 = never\n",
    "        self.dataset = dataset # mnist/fashion_mnist\n",
    "        self.data_dir = data_dir\n",
    "        self.model = model \n",
    "        self.resolution = 299 if self.model == 'inception' else 224\n",
    "        self.cuda = cuda and torch.cuda.is_available()\n",
    "        self.categories = n_categories\n",
    "        if self.categories == 10:\n",
    "            self.classes_txt = 'meta/C10.txt'\n",
    "            self.train_txt = 'meta/train_C10.txt'\n",
    "            self.test_txt = 'meta/test_C10.txt'\n",
    "        else:\n",
    "            self.classes_txt = 'meta/classes.txt'\n",
    "            self.train_txt = 'meta/train.txt'\n",
    "            self.test_txt = 'meta/test.txt'\n",
    "        \n",
    "        self.partition, self.labels = partition_data_and_index_labels(self.data_dir,self.dataset,self.classes_txt,self.train_txt,self.test_txt)\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXEIpyZLil-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2L, 'GPUs available!!!')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count(),\"GPUs available!!!\")\n",
    "else:\n",
    "    print(\"GPU not available!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dV1-9iL-ionf"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch, total_minibatch_count,\n",
    "        train_losses, train_accs):\n",
    "    # Training for a full epoch\n",
    "\n",
    "    model.train()\n",
    "    correct_count, total_loss, total_acc = 0., 0., 0.\n",
    "    progress_bar = tqdm.tqdm(train_loader, desc='Training')\n",
    "    CEloss = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward prediction step\n",
    "        #Inception model needs special handling: refer https://github.com/pytorch/vision/issues/302\n",
    "        if args.model == 'inception':\n",
    "            output,_ = model(data)\n",
    "        else:\n",
    "            output= model(data) \n",
    "        loss = CEloss(output, target)\n",
    "\n",
    "        # Backpropagation step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # The batch has ended, determine the accuracy of the predicted outputs\n",
    "        pred = output.data.max(1)[1]  \n",
    "\n",
    "        # target labels and predictions are categorical values from 0 to 9.\n",
    "        matches = target == pred\n",
    "        accuracy = matches.float().mean()\n",
    "        correct_count += matches.sum()\n",
    "\n",
    "        if args.log_interval != 0 and \\\n",
    "                total_minibatch_count % args.log_interval == 0:\n",
    "\n",
    "            train_losses.append(loss.data[0])\n",
    "            train_accs.append(accuracy.data[0])\n",
    "            \n",
    "        total_loss += loss.data\n",
    "        total_acc += accuracy.data\n",
    "            \n",
    "        progress_bar.set_description(\n",
    "            'Epoch: {} loss: {:.4f}, acc: {:.2f}'.format(\n",
    "                epoch, total_loss / (batch_idx + 1), total_acc / (batch_idx + 1)))\n",
    "        #progress_bar.refresh()\n",
    "\n",
    "        total_minibatch_count += 1\n",
    "\n",
    "    return total_minibatch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLSFEgGnipM3"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nnxt4dycisJ4"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, total_minibatch_count,\n",
    "        val_losses, val_accs):\n",
    "    # Validation Testing\n",
    "    model.eval()\n",
    "    test_loss, correct = 0., 0.\n",
    "    progress_bar = tqdm.tqdm(test_loader, desc='Validation')\n",
    "    CEloss = nn.CrossEntropyLoss()\n",
    "    if epoch==args.epochs:\n",
    "      if args.cuda:\n",
    "        #cum_pred=torch.cuda.LongTensor([])\n",
    "        cum_target=torch.cuda.LongTensor([])\n",
    "        cum_output=torch.cuda.FloatTensor([])\n",
    "      else:\n",
    "        #cum_pred=torch.LongTensor([])\n",
    "        cum_target=torch.LongTensor([])\n",
    "        cum_output=torch.FloatTensor([])\n",
    "    with torch.no_grad():\n",
    "        for data, target in progress_bar:\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            #Inception model needs special handling: refer https://github.com/pytorch/vision/issues/302\n",
    "            if args.model == 'inception':\n",
    "                output,_ = model(data)\n",
    "            else:\n",
    "                output = model(data) \n",
    "            test_loss += CEloss(output, target).data  # sum up batch loss\n",
    "            pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "            if epoch==args.epochs:\n",
    "              cum_output=torch.cat((cum_output, output))\n",
    "              #cum_pred= torch.cat((cum_pred, pred))\n",
    "              cum_target=torch.cat((cum_target, target))\n",
    "            correct += (target == pred).float().sum()\n",
    "    #confusion=confusion_matrix(cum_target.cpu().numpy(),cum_pred.cpu().numpy())\n",
    "    \n",
    "#    print('\\nConfusion_matrix:\\n', confusion)\n",
    "    if epoch==args.epochs:\n",
    "      #cm_analysis(cum_target, cum_pred)\n",
    "      acc1, acc5 = accuracy(cum_output, cum_target, topk=(1, 5))\n",
    "      print ('\\n\\nTop 5 accuracy is  : ', acc5,' while Top 1 accuracy is :', acc1,'\\n')\n",
    "    \n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    \n",
    "    val_losses.append(test_loss)\n",
    "    val_accs.append(acc)\n",
    "    \n",
    "    progress_bar.clear()\n",
    "    progress_bar.write(\n",
    "        '\\nEpoch: {} validation test results - Average val_loss: {:.4f}, val_acc: {}/{} ({:.2f}%)'.format(\n",
    "            epoch, test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ex7ih6bbiuML"
   },
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "def run_experiment(args,resume=0):\n",
    "\n",
    "    total_minibatch_count = 0\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader, test_loader, _, _, run_path = prepare_dataset(args)\n",
    "\n",
    "    \n",
    "    epochs_to_run = args.epochs\n",
    "    \n",
    "\n",
    "    # Choose model\n",
    "    # TODO add all the other models here if their parameter is specified\n",
    "    if args.model == 'default' or args.model == 'P2Q7DefaultChannelsNet':\n",
    "        model = Net()\n",
    "    elif args.model == 'inception':\n",
    "        model = models.inception_v3(pretrained=True)\n",
    "\n",
    "    elif args.model=='ResNet152':\n",
    "        model=models.resnet152(pretrained=True)\n",
    "    elif args.model in globals():\n",
    "        model = globals()[args.model]()\n",
    "    else:\n",
    "        raise ValueError('Unknown model type: ' + args.model)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # Choose optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    elif args.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters())\n",
    "    else:\n",
    "        raise ValueError('Unsupported optimizer: ' + args.optimizer)\n",
    "\n",
    "    # Run the primary training loop, starting with validation accuracy of 0\n",
    "    val_acc = 0\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    last_epoch=1\n",
    "    if resume==1:\n",
    "      path='/checkpoints/' ## change the path to where thw checkpoint file is\n",
    "      path=os.path.join(path + args.model+'_lastest.pkl') \n",
    "      cuda = torch.cuda.is_available()\n",
    "      if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "      else:\n",
    "          # Load GPU model on CPU\n",
    "          checkpoint = torch.load(path,\n",
    "                                  map_location=lambda storage,\n",
    "                                  loc: storage)\n",
    "      model.load_state_dict(checkpoint['state_dict'])\n",
    "      last_epoch = checkpoint['epoch'] +1\n",
    "    \n",
    "\n",
    "    for epoch in range(last_epoch, epochs_to_run + 1):\n",
    "        \n",
    "        # train for 1 epoch\n",
    "        total_minibatch_count = train(model, optimizer, train_loader,\n",
    "                                    epoch, total_minibatch_count,\n",
    "                                    train_losses, train_accs)\n",
    "        # validate progress on test dataset\n",
    "        val_acc = test(model, test_loader, epoch, total_minibatch_count,\n",
    "                       val_losses, val_accs)\n",
    "        path='/checkpoints/'       ##change the path where you wanna save the checkpoint file\n",
    "        state={\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': val_acc,\n",
    "        'args': args\n",
    "        \n",
    "    }\n",
    "        torch.save(state, os.path.join(path + args.model+'_epoch'+str(epoch)+'.pkl'))\n",
    "        torch.save(state, os.path.join(path + args.model+'_lastest.pkl'))\n",
    "    fig, axes = plt.subplots(1,4, figsize=(13,4))\n",
    "    # plot the losses and acc\n",
    "    plt.title(args.name)\n",
    "    axes[0].plot(train_losses)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(train_accs)\n",
    "    axes[1].set_title(\"Acc\")\n",
    "    axes[2].plot(val_losses)\n",
    "    axes[2].set_title(\"Val loss\")\n",
    "    axes[3].plot(val_accs)\n",
    "    axes[3].set_title(\"Val Acc\")\n",
    "    \n",
    "    # Write to csv file\n",
    "    with open(os.path.join(run_path + 'train.csv'), 'w') as f:\n",
    "        csvw = csv.writer(f, delimiter=',')\n",
    "        for loss, acc in zip(train_losses, train_accs):\n",
    "            csvw.writerow((loss, acc))\n",
    "\n",
    "    # Predict and Test\n",
    "    images, labels = next(iter(test_loader))\n",
    "    if args.cuda:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "    output = model(images)\n",
    "    predicted = torch.max(output, 1)[1]\n",
    "    fig, axes = plt.subplots(1,6)\n",
    "    for i, (axis, img, lbl) in enumerate(zip(axes, images, predicted)):\n",
    "        if i > 5:\n",
    "            break\n",
    "        img = img.permute(1,2,0).squeeze()\n",
    "        axis.imshow(img)\n",
    "        axis.set_title(lbl.data)\n",
    "        axis.set_yticklabels([])\n",
    "        axis.set_xticklabels([])\n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRZbpCYWRF-6"
   },
   "outputs": [],
   "source": [
    "#args = Args(model='ResNet152',numclass=101,epochs=30,lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmAvBUYCY0Tm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78aUKgc5Swtb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2732707228ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ResNet152'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_categories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Args' is not defined"
     ]
    }
   ],
   "source": [
    "run_experiment(Args(model='ResNet152',n_categories=101,epochs=30,batch_size=8,lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Resnet152.ipynb",
   "provenance": [
    {
     "file_id": "1qARhvz5iDQUJaPyMJ9QkaSCOhh6it_-v",
     "timestamp": 1542511456466
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
