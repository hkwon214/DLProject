{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_CHhBX6L9B4"
   },
   "source": [
    "# **Fine Tuning and Ensembling Deep Architectures on Large Datasets in Resource Constrained Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9qsxzlgM9Ih"
   },
   "source": [
    "**The code in this file is used for fine tuning Resnet and Inception modules on ETHZ Food-101 dataset and test on UPMC Food-101 dataset. The base code is taken from the Deep Learning Programming Assignments and is modified and updated for the specific use of this project. Most of the rest of the code is our own work and the part where it is not is appropriately acknowledged.**\n",
    "\n",
    "The authors of this file are Amama Mahmood, Heeyeon Kwon and Nafisa Ali Amir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwjhVLprQfeL"
   },
   "source": [
    "**Section A: Prepare the environment**\n",
    "\n",
    "```\n",
    "Get the necessary installations, mount the drive and import packages\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "GWcQZB32ezLu"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Install pytorch and tqdm (if necessary)\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "\n",
    "!pip install Pillow==4.0.0\n",
    "!pip install PIL\n",
    "!pip install image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee9W_AEye9Ay"
   },
   "outputs": [],
   "source": [
    "# Mount your google drive as the data drive\n",
    "# This will require google authorization\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNFsTMhTfF28"
   },
   "outputs": [],
   "source": [
    "# Handle imports\n",
    "\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import torch.utils.data as TUdata\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() \n",
    "                                                     else torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gj3ViqcPQPYe"
   },
   "source": [
    "**Section B: Prepare Data for Food-101**\n",
    "\n",
    "\n",
    "```\n",
    "Upload images from drive.\n",
    "Partition them into training and test (test here is actually the validation set)\n",
    "Prepare the train and test loaders by giving the data path and setting arguments for doing normalisation and image pre-processing\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSU2FCwyfOMT"
   },
   "outputs": [],
   "source": [
    "#Uploading images from the drive\n",
    "def default_loader(path):\n",
    "\treturn Image.open(path).convert('RGB')\n",
    "\n",
    "class FOOD101(TUdata.Dataset):\n",
    "    def __init__(self, root, list_IDs, labels, transform=None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data_dir = os.path.join(root,'food101/images/')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label ##example drive/My Drive/cs482/DL-project/food101/apple_pie/xxxxx.jpg\n",
    "        img = default_loader(self.data_dir + ID[0] + '/' + ID[1] + '.jpg')\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(img)\n",
    "        y = self.labels[ID[0]]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmbJ_Sui5H2f"
   },
   "outputs": [],
   "source": [
    "def partition_data_and_index_labels(data_dir, dataset, classes_txt, train_txt, test_txt):\n",
    "'''Partitioning data into train and test(validation actually) and indexing \n",
    "labels for all the categories'''\n",
    "    labels_file = os.path.join(data_dir,dataset,classes_txt)\n",
    "    labels = {}\n",
    "    '''labels -> {apple_pie : 0, fish_and_chips : ...}'''\n",
    "    with open(labels_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            line = line\n",
    "            labels[line] = idx\n",
    "    \n",
    "    train_file = os.path.join(data_dir,dataset,train_txt)\n",
    "    test_file = os.path.join(data_dir,dataset,test_txt)\n",
    "    partition = defaultdict(list)\n",
    "    \n",
    "    '''partition -> {'train : [[apple_pie, xxxxx.jpg], [apple_pie, xxyyyy.jpg], \n",
    "    [fish_and_chips, aaaabb.jpg]], test: [[apple_pie, abcdabcd.jpg]]}}'''\n",
    "    \n",
    "    with open(train_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            ID = line.split('/')\n",
    "            partition['train'].append([ID[0], ID[1]])\n",
    "        \n",
    "\n",
    "    with open(test_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            ID = line.split('/')\n",
    "            partition['test'].append([ID[0], ID[1]])\n",
    "            \n",
    "    return partition, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2D5b456XHfR3"
   },
   "outputs": [],
   "source": [
    "# Create the train and test loaders\n",
    "def prepare_dataset(args):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "    \n",
    "    \n",
    "    train_dataset = FOOD101(args.data_dir, args.partition['train'], args.labels, transform=transforms.Compose([transforms.RandomResizedCrop(args.resolution),\n",
    "                                                                                                               transforms.RandomHorizontalFlip(),\n",
    "                                                                                                               transforms.ToTensor(),\n",
    "                                                                                                               transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                                                                                                                                  std = [ 0.229, 0.224, 0.225 ]) \n",
    "                                                                                                            ]))\n",
    "\n",
    "\n",
    "    train_loader = TUdata.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "    test_dataset = FOOD101(args.data_dir, args.partition['test'], args.labels, transform=transforms.Compose([transforms.RandomResizedCrop(args.resolution),\n",
    "                                                                                                             transforms.RandomHorizontalFlip(),\n",
    "                                                                                                             transforms.ToTensor(),\n",
    "                                                                                                             transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                                                                                                                                  std = [ 0.229, 0.224, 0.225 ]) \n",
    "                                                                                                            ]))\n",
    "\n",
    "    test_loader = TUdata.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    \n",
    "    def time_stamp(fname, fmt='%m-%d-%H-%M_{fname}'):\n",
    "        return datetime.datetime.now().strftime(fmt).format(fname=fname)\n",
    "        \n",
    "    training_run_name = time_stamp(args.dataset + '_' + args.name)\n",
    "    training_run_dir = os.path.join(args.data_dir, 'run_dir', training_run_name)\n",
    "    \n",
    "    if not os.path.exists(training_run_dir):\n",
    "        os.makedirs(training_run_dir)\n",
    "    \n",
    "    return train_loader, test_loader, train_dataset, test_dataset, training_run_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IE7o90ZbRTW4"
   },
   "source": [
    "**Section C: Visualise Results in the form of Confusion Matrix**\n",
    "\n",
    "\n",
    "```\n",
    "The function cm_analysis is taken from ...\n",
    "It allows us to visualise the results. This has been used for 10 category classification only.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atKckj1f585j"
   },
   "outputs": [],
   "source": [
    "#Function used for creating confusion matrix\n",
    "#Acknowledgement to: https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "#Updated to meet requirements \n",
    "def cm_analysis(y_true, y_pred, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    labels=map(str, range(9 + 1))\n",
    "    labels_file = os.path.join(args.data_dir,args.dataset,'meta/C10.txt')\n",
    "    ymap = {}\n",
    "    '''labels -> {apple_pie : 0, fish_and_chips : 4}'''\n",
    "    with open(labels_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            line = line\n",
    "            ymap[idx] = line\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[np.int(yi)] for yi in y_pred]\n",
    "        y_true = [ymap[np.int(yi)] for yi in y_true]\n",
    "        labels = [ymap[np.int(yi)] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
    "    plt.show\n",
    "    #plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6pySlY3R4Kz"
   },
   "source": [
    "**Section D: Parameters**\n",
    "\n",
    "\n",
    "```\n",
    "The class Args handles all parameters. \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkKaDdvBt90u"
   },
   "outputs": [],
   "source": [
    "# The Args object will contain all of our parameters\n",
    "# If you want to run with different arguments, create another Args object\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self, name='food101', batch_size=64, test_batch_size=1000,\n",
    "            epochs=50, lr=0.0001, optimizer='adam', momentum=0.5,\n",
    "            seed=1, log_interval=100, dataset='food101', n_categories=101,\n",
    "            data_dir='drive/My Drive/DL-project/', model='ResNet152',\n",
    "            cuda=True):\n",
    "        self.name = name # name for this training run. Don't use spaces.\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size # Input batch size for testing\n",
    "        self.epochs = epochs # Number of epochs to train\n",
    "        self.lr = lr # Learning rate\n",
    "        self.optimizer = optimizer # sgd/p1sgd/adam/rms_prop\n",
    "        self.momentum = momentum # SGD Momentum\n",
    "        self.seed = seed # Random seed\n",
    "        self.log_interval = log_interval # Batches to wait before logging\n",
    "                                     # detailed status. 0 = never\n",
    "        self.dataset = dataset # mnist/fashion_mnist\n",
    "        self.data_dir = data_dir\n",
    "        self.model = model \n",
    "        self.resolution = 299 if self.model == 'inception' else 224\n",
    "        self.cuda = cuda and torch.cuda.is_available()\n",
    "        self.categories = n_categories\n",
    "        if self.categories == 10:\n",
    "            self.classes_txt = 'meta/C10.txt'\n",
    "            self.train_txt = 'meta/train_C10.txt'\n",
    "            self.test_txt = 'meta/test_C10.txt'\n",
    "        else:\n",
    "            self.classes_txt = 'meta/classes.txt'\n",
    "            self.train_txt = 'meta/train.txt'\n",
    "            self.test_txt = 'meta/test.txt'\n",
    "        \n",
    "        self.partition, self.labels = partition_data_and_index_labels(self.data_dir,self.dataset,self.classes_txt,self.train_txt,self.test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYq13dml5wtK"
   },
   "outputs": [],
   "source": [
    "#To print the number of GPU's available\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count(),\"GPUs available!!!\")\n",
    "else:\n",
    "    print(\"GPU not available!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbAei-doSOrl"
   },
   "source": [
    "**Section E: Training, Validation and Accuracy Computation**\n",
    "```  \n",
    "Models supported are Inception-v3, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152\n",
    "\n",
    "Optimizers supported are Adam, SGD and RMSProp\n",
    "\n",
    "The function run_experiment calls train and test functions for training and validation respectively for every epoch. After each epoch, checkpoints are created by saving the model parameters.\n",
    "\n",
    "The argument resume is added to run_experiment, which allows training in parts and resuming from the last saved checkpoint when set to 1.\n",
    "\n",
    "Once max epochs is reached, the final checkpoint is saved, top-1 and top-5 accuracies are computed and training and validation accuracy plots are generated and saved.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1_JPwmU8fId"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch, total_minibatch_count,\n",
    "        train_losses, train_accs,args):\n",
    "    # Training for a full epoch\n",
    "\n",
    "    model.train()\n",
    "    correct_count, total_loss, total_acc = 0., 0., 0.\n",
    "    progress_bar = tqdm.tqdm(train_loader, desc='Training')\n",
    "    CEloss = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward prediction step\n",
    "        #Inception model needs special handling: refer https://github.com/pytorch/vision/issues/302\n",
    "        if args.model == 'inception':\n",
    "            output,_ = model(data)\n",
    "        else:\n",
    "            output= model(data) \n",
    "        loss = CEloss(output, target)\n",
    "\n",
    "        # Backpropagation step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # The batch has ended, determine the accuracy of the predicted outputs\n",
    "        pred = output.data.max(1)[1]  \n",
    "\n",
    "        # target labels and predictions are categorical values from 0 to 9.\n",
    "        matches = target == pred\n",
    "        accuracy = matches.float().mean()\n",
    "        correct_count += matches.sum()\n",
    "\n",
    "        if args.log_interval != 0 and \\\n",
    "                total_minibatch_count % args.log_interval == 0:\n",
    "\n",
    "            train_losses.append(loss.data[0])\n",
    "            train_accs.append(accuracy.data[0])\n",
    "            \n",
    "        total_loss += loss.data\n",
    "        total_acc += accuracy.data\n",
    "            \n",
    "        progress_bar.set_description(\n",
    "            'Epoch: {} loss: {:.4f}, acc: {:.2f}'.format(\n",
    "                epoch, total_loss / (batch_idx + 1), total_acc / (batch_idx + 1)))\n",
    "        #progress_bar.refresh()\n",
    "\n",
    "        total_minibatch_count += 1\n",
    "\n",
    "    return total_minibatch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pT_DN7LE5IbI"
   },
   "outputs": [],
   "source": [
    "#Computes Top 1 and Top k accuracy\n",
    "#Acknowlegment to: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNvuY-jMAsrX"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch, total_minibatch_count,\n",
    "        val_losses, val_accs,args):\n",
    "    # Validation Testing\n",
    "    model.eval()\n",
    "    test_loss, correct = 0., 0.\n",
    "    progress_bar = tqdm.tqdm(test_loader, desc='Validation')\n",
    "    CEloss = nn.CrossEntropyLoss()\n",
    "    #if epoch==args.epochs:\n",
    "    if args.cuda:\n",
    "      cum_pred=torch.cuda.LongTensor([])\n",
    "      cum_target=torch.cuda.LongTensor([])\n",
    "      cum_output=torch.cuda.FloatTensor([])\n",
    "    else:\n",
    "      cum_pred=torch.LongTensor([])\n",
    "      cum_target=torch.LongTensor([])\n",
    "      cum_output=torch.FloatTensor([])\n",
    "    with torch.no_grad():\n",
    "        for data, target in progress_bar:\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            #Inception model needs special handling: refer https://github.com/pytorch/vision/issues/302\n",
    "            if args.model == 'inception':\n",
    "                output = model(data)\n",
    "            else:\n",
    "                output = model(data) \n",
    "            test_loss += CEloss(output, target).data  # sum up batch loss\n",
    "            pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "            if epoch==args.epochs:\n",
    "              cum_output=torch.cat((cum_output, output))\n",
    "              cum_pred= torch.cat((cum_pred, pred))\n",
    "              cum_target=torch.cat((cum_target, target))\n",
    "            correct += (target == pred).float().sum()\n",
    "    \n",
    "#    print('\\nConfusion_matrix:\\n', confusion)\n",
    "    if epoch==args.epochs:\n",
    "      #cm_analysis(cum_target, cum_pred)\n",
    "      acc1, acc5 = accuracy(cum_output, cum_target, topk=(1, 5))\n",
    "      print ('\\n\\nTop 5 accuracy is  : ', acc5,' while Top 1 accuracy is :', acc1,'\\n')\n",
    "      \n",
    "      ## only uncomment when target file needs to be saved for upmc or food101 testset, edit accordingly\n",
    "      #path2='/content/drive/My Drive/DL-project/probabilities/'\n",
    "      #cum_target=cum_target.cpu().numpy()\n",
    "      #df = pd.DataFrame(cum_target)\n",
    "      #df.to_csv(os.path.join(path2+'upmc_'+args.model+'_target.csv'))\n",
    "      #print('\\npred: ',cum_pred, '\\ntarget: ', cum_target)\n",
    "    \n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    \n",
    "    val_losses.append(test_loss)\n",
    "    val_accs.append(acc)\n",
    "    \n",
    "    progress_bar.clear()\n",
    "    progress_bar.write(\n",
    "        '\\nEpoch: {} validation test results - Average val_loss: {:.4f}, val_acc: {}/{} ({:.2f}%)'.format(\n",
    "            epoch, test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    return acc,cum_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTF7AuoADfj7"
   },
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "def run_experiment(args,resume=0):\n",
    "\n",
    "    total_minibatch_count = 0\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader, test_loader, _, _, run_path = prepare_dataset(args)\n",
    "\n",
    "    \n",
    "    epochs_to_run = args.epochs\n",
    "    \n",
    "    \n",
    "\n",
    "    # Choose model\n",
    "    if args.model == 'inception':\n",
    "        model = models.inception_v3(pretrained=True)\n",
    "    elif args.model == 'ResNet18':\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif args.model=='ResNet34':\n",
    "        model=models.resnet34(pretrained=True)\n",
    "    elif args.model=='ResNet50':\n",
    "        model=models.resnet50(pretrained=True)\n",
    "    elif args.model=='ResNet101':\n",
    "        model=models.resnet101(pretrained=True)\n",
    "    elif args.model=='ResNet152':\n",
    "        model=models.resnet152(pretrained=True)\n",
    "    elif args.model in globals():\n",
    "        model = globals()[args.model]()\n",
    "    else:\n",
    "        raise ValueError('Unknown model type: ' + args.model)\n",
    "    ##updating fc layer to make 101 classes rather than 1000\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 101)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # Choose optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    elif args.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters())\n",
    "    else:\n",
    "        raise ValueError('Unsupported optimizer: ' + args.optimizer)\n",
    "\n",
    "    # Run the primary training loop, starting with validation accuracy of 0\n",
    "    val_acc = 0\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    last_epoch=1\n",
    "    if resume==1:\n",
    "      path='/content/drive/My Drive/DL-project/checkpoints/' ## change the path to where thw checkpoint file is\n",
    "      path=os.path.join(path + args.model+'_lastest.pkl') \n",
    "      cuda = torch.cuda.is_available()\n",
    "      if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "      else:\n",
    "          # Load GPU model on CPU\n",
    "          checkpoint = torch.load(path,\n",
    "                                  map_location=lambda storage,\n",
    "                                  loc: storage)\n",
    "      model.load_state_dict(checkpoint['state_dict'])\n",
    "      last_epoch = checkpoint['epoch'] +1\n",
    "    \n",
    "\n",
    "    for epoch in range(last_epoch, epochs_to_run + 1):\n",
    "        \n",
    "        # train for 1 epoch\n",
    "        total_minibatch_count = train(model, optimizer, train_loader,\n",
    "                                    epoch, total_minibatch_count,\n",
    "                                    train_losses, train_accs,args)\n",
    "        # validate progress on test dataset\n",
    "        val_acc,_= test(model, test_loader, epoch, total_minibatch_count,\n",
    "                       val_losses, val_accs)\n",
    "        path='/content/drive/My Drive/DL-project/checkpoints/'       ##change the path where you wanna save the checkpoint file\n",
    "        state={\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': val_acc,\n",
    "        'args': args\n",
    "        \n",
    "    }\n",
    "        torch.save(state, os.path.join(path + args.model+'_epoch'+str(epoch)+'.pkl'))\n",
    "        torch.save(state, os.path.join(path + args.model+'_lastest1.pkl'))\n",
    "    fig, axes = plt.subplots(1,4, figsize=(13,4))\n",
    "    # plot the losses and acc\n",
    "    plt.title(args.name)\n",
    "    axes[0].plot(train_losses)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(train_accs)\n",
    "    axes[1].set_title(\"Acc\")\n",
    "    axes[2].plot(val_losses)\n",
    "    axes[2].set_title(\"Val loss\")\n",
    "    axes[3].plot(val_accs)\n",
    "    axes[3].set_title(\"Val Acc\")\n",
    "    \n",
    "    # Write to csv file\n",
    "    with open(os.path.join(run_path + 'train.csv'), 'w') as f:\n",
    "        csvw = csv.writer(f, delimiter=',')\n",
    "        for loss, acc in zip(train_losses, train_accs):\n",
    "            csvw.writerow((loss, acc))\n",
    "\n",
    "    # Predict and Test\n",
    "    images, labels = next(iter(test_loader))\n",
    "    if args.cuda:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "    output = model(images)\n",
    "    predicted = torch.max(output, 1)[1]\n",
    "    fig, axes = plt.subplots(1,6)\n",
    "    for i, (axis, img, lbl) in enumerate(zip(axes, images, predicted)):\n",
    "        if i > 5:\n",
    "            break\n",
    "        img = img.permute(1,2,0).squeeze()\n",
    "        axis.imshow(img)\n",
    "        axis.set_title(lbl.data)\n",
    "        axis.set_yticklabels([])\n",
    "        axis.set_xticklabels([])\n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-W2600H-UNEu"
   },
   "source": [
    "**The command to start the training**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78aUKgc5Swtb"
   },
   "outputs": [],
   "source": [
    "run_experiment(Args(model='ResNet152',n_categories=101,epochs=10,batch_size=8,lr=0.0001,optimizer='adam'),resume=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OS2ZL9nyVSRi"
   },
   "source": [
    "**Section F: Ensembling Resnet-152 and Inception v3**\n",
    "```  \n",
    "This part of code used to compute the validation accuracy on last epoch for unshuffled test images and saves the output into .csv files along with target labels which are needed for ensembling later on.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXkmpqfJouPA"
   },
   "outputs": [],
   "source": [
    "model='inception'\n",
    "path='/content/drive/My Drive/DL-project/checkpoints/' ## change the path to where thw checkpoint file is\n",
    "path2='/content/drive/My Drive/DL-project/probabilities/'\n",
    "path=os.path.join(path + model+'_latest.pkl')                       ## change the filename if needed\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    checkpoint = torch.load(path)\n",
    "else:\n",
    "    # Load GPU model on CPU\n",
    "    checkpoint = torch.load(path,\n",
    "                            map_location=lambda storage,\n",
    "                            loc: storage)\n",
    "args= checkpoint['args']\n",
    "args.data_dir='drive/My Drive/DL-project/'\n",
    "args.epochs=checkpoint['epoch']\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "test_dataset = FOOD101(args.data_dir, args.partition['test'], args.labels, transform=transforms.Compose([transforms.RandomResizedCrop(args.resolution),\n",
    "                                                                                                         transforms.RandomHorizontalFlip(),\n",
    "                                                                                                         transforms.ToTensor(),\n",
    "                                                                                                         transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                                                                                                                            std = [ 0.229, 0.224, 0.225 ]) \n",
    "                                                                                                        ]))\n",
    "test_loader = TUdata.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "# Choose model\n",
    "if args.model == 'default' or args.model == 'P2Q7DefaultChannelsNet':\n",
    "    model = Net()\n",
    "elif args.model == 'inception':\n",
    "    model = models.inception_v3(pretrained=True)\n",
    "elif args.model == 'ResNet18':\n",
    "    model = models.resnet18(pretrained=True)\n",
    "elif args.model=='ResNet34':\n",
    "    model=models.resnet34(pretrained=True)\n",
    "elif args.model=='ResNet50':\n",
    "    model=models.resnet50(pretrained=True)\n",
    "elif args.model=='ResNet101':\n",
    "    model=models.resnet101(pretrained=True)\n",
    "elif args.model=='ResNet152':\n",
    "    model=models.resnet152(pretrained=True)\n",
    "elif args.model in globals():\n",
    "    model = globals()[args.model]()\n",
    "else:\n",
    "    raise ValueError('Unknown model type: ' + args.model)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 101)\n",
    "#print(model)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model).cuda()  #comment it out while loading Resnet-152 or if key mismatch error occurs in state_dict\n",
    "epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "total_minibatch_count= 0\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(path, checkpoint['epoch']),'\\n')\n",
    "\n",
    "\n",
    "val_losses, val_accs = [], []\n",
    "val_acc,output = test(model, test_loader, epoch, total_minibatch_count,\n",
    "                   val_losses, val_accs,args)\n",
    "\n",
    "path2='/content/drive/My Drive/DL-project/probabilities/'\n",
    "output=output.cpu().numpy()\n",
    "df = pd.DataFrame(output)\n",
    "df.to_csv(os.path.join(path2+'inception_latest.csv'))     #change the file to be saved to according to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6q0whs0bFK9"
   },
   "outputs": [],
   "source": [
    "path2='/content/drive/My Drive/DL-project/probabilities/'\n",
    "print('loading inception')\n",
    "prob_inc = pd.read_csv(os.path.join(path2+'inception_latest.csv'), sep=\",\");\n",
    "prob_inc=prob_inc.values[:,1:]\n",
    "prob_inc=torch.tensor(prob_inc)\n",
    "prob_inc=(prob_inc-prob_inc.min(1,keepdim=True)[0]) / (prob_inc.max(1, keepdim=True)[0]-prob_inc.min(1,keepdim=True)[0])\n",
    "\n",
    "print('loading residual')\n",
    "prob_res = pd.read_csv(os.path.join(path2+'resnet_latest_10epochs.csv'), sep=\",\");\n",
    "prob_res=prob_res.values[:,1:]\n",
    "prob_res=torch.tensor(prob_res)\n",
    "prob_res=(prob_res-prob_res.min(1,keepdim=True)[0]) / (prob_res.max(1, keepdim=True)[0]-prob_res.min(1,keepdim=True)[0])\n",
    "\n",
    "print('loading_target')\n",
    "target = pd.read_csv(os.path.join(path2+'_inception_target.csv'), sep=\",\");\n",
    "target=torch.cuda.LongTensor(target.values[:,1:])\n",
    "#Ensemble\n",
    "final_prob=prob_inc*0.5+prob_res*0.5\n",
    "final_prob=torch.tensor(final_prob)\n",
    "\n",
    "acc1, acc5 = accuracy(final_prob, target, topk=(1, 5))\n",
    "\n",
    "print(acc1,acc5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed_2K8_gjPBj"
   },
   "outputs": [],
   "source": [
    "#Uploading images from drive\n",
    "class UPMC_FOOD101(TUdata.Dataset):\n",
    "    def __init__(self, root, list_IDs, labels, transform=None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data_dir = os.path.join(root,'upmc_food101/images/')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label ##example drive/My Drive/cs482/DL-project/upmc_food101/train/apple_pie/xxxxx.jpg\n",
    "        img = default_loader(self.data_dir + ID[0] + '/' + ID[1] + '/' + ID[2])\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(img)\n",
    "        y = self.labels[ID[1]]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yF8EGR-hVrDU"
   },
   "source": [
    "**Section G: Evaluation on UPMC Food-101**\n",
    "```  \n",
    "Prepare dataset by uploading images and creating the test loader in a similar manner as was done for Food-101\n",
    "\n",
    "The same models are supported here.\n",
    "\n",
    "There is no training or validation here. Only one round of test which gives a prediction of the class associated with the image.\n",
    "\n",
    "It eventually prints the top-1 and top-5 accuracy.\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8tddpzqYbku"
   },
   "outputs": [],
   "source": [
    "def evaluate_upmc_food101(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    #Give the path of the image list file (all images from upmc)\n",
    "    upmc_test_file = os.path.join(args.data_dir,'upmc_food101/images/upmc_testset.txt')\n",
    "    partition = defaultdict(list)\n",
    "    with open(upmc_test_file,'r') as rf:\n",
    "        for idx, line in enumerate(rf.readlines()):\n",
    "            line = line.strip()\n",
    "            ID = line.split('/')\n",
    "            partition['upmc_test'].append([ID[0], ID[1], ID[2]])\n",
    "\n",
    "    upmc_test_dataset = UPMC_FOOD101(args.data_dir, partition['upmc_test'], args.labels, transform=transforms.Compose([transforms.RandomResizedCrop(args.resolution),\n",
    "                                                                                                               transforms.RandomHorizontalFlip(),\n",
    "                                                                                                               transforms.ToTensor(),\n",
    "                                                                                                               transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                                                                                                                                  std = [ 0.229, 0.224, 0.225 ]) \n",
    "                                                                                                            ]))\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "    upmc_test_loader = TUdata.DataLoader(upmc_test_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # Choose model\n",
    "    if args.model == 'inception':\n",
    "        model = models.inception_v3(pretrained=True)\n",
    "    elif args.model == 'ResNet18':\n",
    "        model = models.resnet18(pretrained=True)\n",
    "    elif args.model=='ResNet34':\n",
    "        model=models.resnet34(pretrained=True)\n",
    "    elif args.model=='ResNet50':\n",
    "        model=models.resnet50(pretrained=True)\n",
    "    elif args.model=='ResNet101':\n",
    "        model=models.resnet101(pretrained=True)\n",
    "    elif args.model=='ResNet152':\n",
    "        model=models.resnet152(pretrained=True)\n",
    "    elif args.model in globals():\n",
    "        model = globals()[args.model]()\n",
    "    else:\n",
    "        raise ValueError('Unknown model type: ' + args.model)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 101)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()         #comment out in case of resnet or if error is in loading state_dict\n",
    "    path = os.path.join(args.data_dir, 'checkpoints', args.model + '_latest.pkl')\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(path,\n",
    "                            map_location=lambda storage,\n",
    "                            loc: storage)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(path, checkpoint['epoch']),'\\n')\n",
    "\n",
    "\n",
    "    # validate progress on test dataset\n",
    "    test_losses, test_accs = [],[]\n",
    "    test_acc,output = test(model, upmc_test_loader, args.epochs, 0,\n",
    "                       test_losses, test_accs,args)\n",
    "    path2='/content/drive/My Drive/DL-project/probabilities/'\n",
    "    output=output.cpu().numpy()\n",
    "    df = pd.DataFrame(output)\n",
    "    df.to_csv(os.path.join(path2+'upmc_inception_latest.csv')) # change as needed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAgNCvAmWggO"
   },
   "source": [
    "**Command to run the evaluation code**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXLl2MVxi1X_"
   },
   "outputs": [],
   "source": [
    "evaluate_upmc_food101(Args(model='inception',n_categories=101,epochs=30,batch_size=64,lr=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_LhINhYqEdj"
   },
   "source": [
    "** Section H: Ensembling test result on UPMC-food101 dataset**\n",
    "\n",
    "Loading csv files and then ensembling the output of two models to give results on testset of UPMC-food101.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwK_mXTNyMsu"
   },
   "outputs": [],
   "source": [
    "\n",
    "path2='/content/drive/My Drive/DL-project/probabilities/'\n",
    "\n",
    "print('loading inception')\n",
    "prob_inc = pd.read_csv(os.path.join(path2+'upmc_inception_latest.csv'), sep=\",\");\n",
    "prob_inc=prob_inc.values[:,1:]\n",
    "prob_inc=torch.tensor(prob_inc)\n",
    "prob_inc=(prob_inc-prob_inc.min(1,keepdim=True)[0]) / (prob_inc.max(1, keepdim=True)[0]-prob_inc.min(1,keepdim=True)[0])\n",
    "\n",
    "print('loading residual')\n",
    "prob_res = pd.read_csv(os.path.join(path2+'umpc_resnet_latest.csv'), sep=\",\");\n",
    "prob_res=prob_res.values[:,1:]\n",
    "prob_res=torch.tensor(prob_res)\n",
    "prob_res=(prob_res-prob_res.min(1,keepdim=True)[0]) / (prob_res.max(1, keepdim=True)[0]-prob_res.min(1,keepdim=True)[0])\n",
    "\n",
    "print('loading_target')\n",
    "target = pd.read_csv(os.path.join(path2+'upmc_inception_target.csv'), sep=\",\");\n",
    "target=torch.cuda.LongTensor(target.values[:,1:])\n",
    "#Ensemble\n",
    "final_prob=prob_inc*0.5+prob_res*0.5\n",
    "final_prob=torch.tensor(final_prob)\n",
    "\n",
    "acc1, acc5 = accuracy(final_prob, target, topk=(1, 5))\n",
    "\n",
    "print(acc1,acc5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNetInceptiononFood101.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
